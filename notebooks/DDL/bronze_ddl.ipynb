{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9efd483-6176-4f22-9872-3d4b21b4611f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze DDL\n",
    "## Working approach\n",
    "For sake of semplicity, I created 4 bronze tables in bronze schema by uploading the CSV file directly. You can scroll to the end of the notebook for some additional input on how I would have managed the ingestion from a Postgre SQL instance.\n",
    "\n",
    "The script below requires an existing catalog (ideally created with Terraform script but more likely manually if the platform is not already in a very mature state) and contains the creation statement of the schema and the 4 tables. \n",
    "It can be run during deployment through deployment pipeline execution as the environment is a parameter passed.\n",
    "I did not change the data types to the proper one in this phase (it will be performed during bronze-->silver transformation, more reason for that at the end), just uploaded the data as it is in the csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "789562a0-9743-4e3d-b9a0-f22b5051b3ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"env\", \"dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "983e0d78-264e-405e-82cf-37e5d54aaad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "environment = dbutils.widgets.get('env')\n",
    "catalog_name = \"use_case_\" + environment\n",
    "schema_name = \"bronze\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1756d9f3-2f5f-44f6-92be-2ad4f1bb08bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}\")\n",
    "# For automatic execution, we can add commands for granting/managing privileges on schema AND tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd5edbf9-ba2a-432f-b253-40a2a64f074f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"CREATE TABLE {catalog_name}.{schema_name}.teams (\n",
    "  team_id STRING,\n",
    "  team_activity STRING,\n",
    "  country_code STRING,\n",
    "  created_at STRING)\n",
    "USING delta\n",
    "TBLPROPERTIES (\n",
    "  'delta.minReaderVersion' = '1',\n",
    "  'delta.minWriterVersion' = '2')\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74dff3f2-4ee2-4fb8-b75f-ff3f64562290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"CREATE TABLE {catalog_name}.{schema_name}.memberships (\n",
    "  membership_id STRING,\n",
    "  group_id STRING,\n",
    "  role_title STRING,\n",
    "  joined_at STRING)\n",
    "USING delta\n",
    "TBLPROPERTIES (\n",
    "  'delta.minReaderVersion' = '1',\n",
    "  'delta.minWriterVersion' = '2')\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0379999-c307-4be8-9bdb-de29ff6a4039",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"CREATE TABLE {catalog_name}.{schema_name}.events (\n",
    "  event_id STRING,\n",
    "  team_id STRING,\n",
    "  event_start STRING,\n",
    "  event_end STRING,\n",
    "  latitude DOUBLE,\n",
    "  longitude DOUBLE,\n",
    "  created_at STRING)\n",
    "USING delta\n",
    "TBLPROPERTIES (\n",
    "  'delta.minReaderVersion' = '1',\n",
    "  'delta.minWriterVersion' = '2')\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1bf0ae5-ce15-4f14-8a71-41c054d6b8f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"CREATE TABLE {catalog_name}.{schema_name}.event_rsvps (\n",
    "  event_rsvp_id STRING,\n",
    "  event_id STRING,\n",
    "  membership_id STRING,\n",
    "  rsvp_status BIGINT,\n",
    "  responded_at STRING)\n",
    "USING delta\n",
    "TBLPROPERTIES (\n",
    "  'delta.minReaderVersion' = '1',\n",
    "  'delta.minWriterVersion' = '2')\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25d1fc25-19c8-4e59-b0c5-1a3538403876",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Lakehouse federation for PostGreSQL sources\n",
    "\n",
    "In a real case scenario (databricks enviroment with UC enabled, cloud instance for the database), to approach this use case I would use the \"relatively new\" Lakehouse Federation framework.\n",
    "\n",
    "In this approach, you create a connection object directly in UC:\n",
    "\n",
    "`CREATE CONNECTION <connection-name> TYPE postgresql\n",
    "OPTIONS (\n",
    "  host '<hostname>',\n",
    "  port '<port>',\n",
    "  user secret ('<secret-scope>','<secret-key-user>'),\n",
    "  password secret ('<secret-scope>','<secret-key-password>')\n",
    ")`\n",
    "\n",
    "and a foreign catalog using that connection to \"federate\" the postgre sql instance under Databricks UC governance capabilities.\n",
    "\n",
    "`CREATE FOREIGN CATALOG [IF NOT EXISTS] <catalog-name> USING CONNECTION <connection-name>\n",
    "OPTIONS (database '<database-name>')`\n",
    "\n",
    "\n",
    "with this in place, it would be possible directly query the data in posgre by creating a materialized view or using the table under the foreign catalog as sources for a DLT pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d291746-4e6b-49d6-b67f-30549afb3888",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4958936613174479,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_ddl",
   "widgets": {
    "env": {
     "currentValue": "dev",
     "nuid": "201670a4-7d82-4d1f-af50-9ae53cc67b61",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dev",
      "label": null,
      "name": "env",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dev",
      "label": null,
      "name": "env",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
